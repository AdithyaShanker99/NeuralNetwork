{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
      "/Users/adithyashanker/anaconda3/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym\n",
    "from IPython import display\n",
    "import time as t\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "observations = []\n",
    "rewards = []\n",
    "\n",
    "\n",
    "for episode in range(episodes) :\n",
    "    initial_state = env.reset()\n",
    "    ended = False\n",
    "    rewardsum = 0\n",
    "    while not ended :\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, ended, info = env.step(action)\n",
    "        #print(env.observation_space)\n",
    "        observations.append(obs)\n",
    "        rewardsum += reward\n",
    "        #print(rewardsum)\n",
    "    #t.sleep(5)\n",
    "\n",
    "velocities = []\n",
    "ang_velocities = []\n",
    "for observation in observations :\n",
    "    velocities.append(observation[1])\n",
    "    ang_velocities.append(observation[3])\n",
    "\n",
    "print (f'min velocity : {np.min(velocities)}, max velocity : {np.max(velocities)}\\n')\n",
    "print (f'min ang_velocity : {np.min(ang_velocities)}, max ang_velocity : {np.max(ang_velocities)}')\n",
    "      \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "\n",
    "\n",
    "for episode in range(episodes) :\n",
    "    initial_state = env.reset()\n",
    "    ended = False\n",
    "    rewardsum = 0\n",
    "    while not ended :\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, ended, info = env.step(action)\n",
    "        #print(env.observation_space)\n",
    "        observations.append(obs)\n",
    "        rewardsum += reward\n",
    "        #print(rewardsum)\n",
    "    t.sleep(5)\n",
    "\n",
    "velocities = []\n",
    "ang_velocities = []\n",
    "for observation in observations :\n",
    "    velocities.append(observation[1])\n",
    "    ang_velocities.append(observation[3])\n",
    "\n",
    "print (f'min velocity : {np.min(velocities)}, max velocity : {np.max(velocities)}\\n')\n",
    "print (f'min ang_velocity : {np.min(ang_velocities)}, max ang_velocity : {np.max(ang_velocities)}')\n",
    "      \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapStateSpace(obs):\n",
    "\n",
    "    max_position = 2.4 \n",
    "    max_velocity = 5   \n",
    "    max_angle = np.pi   \n",
    "    max_angular_velocity = 4  \n",
    "\n",
    "    # Normalize and discretize \n",
    "    a = np.round((obs[0] / max_position) * 10)  # Discretize position into 10 bins\n",
    "    b = np.round((obs[1] / max_velocity) * 15)  # Discretize velocity into 10 bins\n",
    "    c = np.round((obs[2] / max_angle) * 36)     # Discretize angle into 18 bins (~20 degrees each)\n",
    "    d = np.round((obs[3] / max_angular_velocity) * 15)  # Discretize angular velocity into 10 bins\n",
    "\n",
    "    return int(10*a + 10*b + 30*c + 10*d)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_states = 4000  \n",
    "n_actions = 2\n",
    "\n",
    "Q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_factor = 1\n",
    "exploration_prob = 0.2\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    current_state = env.reset()\n",
    "    #env.render()\n",
    "    ended = False\n",
    "    rewardsum = 0\n",
    "\n",
    "\n",
    "    while not ended :\n",
    "\n",
    "        if np.random.rand() < exploration_prob:\n",
    "            action = env.action_space.sample()  \n",
    "        else:\n",
    "            action = np.argmax(Q_table[mapStateSpace(current_state)])  \n",
    "        \n",
    "        next_state, reward, ended, info = env.step(action)\n",
    "\n",
    "        next_max = np.max(Q_table[mapStateSpace(next_state)])\n",
    "        td_target = reward + discount_factor * next_max\n",
    "        td_error = td_target - Q_table[mapStateSpace(current_state), action]\n",
    "        Q_table[mapStateSpace(current_state), action] += learning_rate * td_error\n",
    "        rewardsum += reward\n",
    "\n",
    "        current_state = next_state \n",
    "    #print(rewardsum)\n",
    "    \n",
    "\n",
    "print(\"Learned Q-table:\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 2000\n",
    "rewards = []\n",
    "\n",
    "\n",
    "for episode in range(episodes) :\n",
    "    state = env.reset()\n",
    "    ended = False\n",
    "    rewardsum = 0\n",
    "    while not ended :\n",
    "        env.render()\n",
    "        action = np.argmax(Q_table[mapStateSpace(state)])\n",
    "        state, reward, ended, info = env.step(action)\n",
    "        rewardsum += reward\n",
    "    #print(rewardsum)\n",
    "    #t.sleep(1)\n",
    "    rewards.append(rewardsum)\n",
    "\n",
    "print(np.max(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(rewards)), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features=8, h1=64, h2=64, h3=64, out_features=4) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features,h1)\n",
    "        self.fc2 = nn.Linear(h1,h2)\n",
    "        self.fc3 = nn.Linear(h2, h3)\n",
    "        self.out = nn.Linear(h3, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.dropout(x, 0.2)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x = F.dropout(x, 0.2)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "policy_network = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def epsilon_decay(epsilon_start, epsilon_final, steps, epsilon_decay):\n",
    "    epsilon = epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * steps / epsilon_decay)\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sy/2jzm18gs78s48trcqgthb1xh0000gn/T/ipykernel_62032/107616033.py:73: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  batch_states = torch.tensor([transition[0] for transition in minibatch], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(79.3885, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9302, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5425, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6607, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0427, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7811, grad_fn=<MseLossBackward0>)\n",
      "tensor(12.0813, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7673, grad_fn=<MseLossBackward0>)\n",
      "tensor(286.3869, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7295, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.7655, grad_fn=<MseLossBackward0>)\n",
      "tensor(27.8934, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4923, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6464, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8472, grad_fn=<MseLossBackward0>)\n",
      "tensor(47.9638, grad_fn=<MseLossBackward0>)\n",
      "tensor(163.0980, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6291, grad_fn=<MseLossBackward0>)\n",
      "tensor(441.7270, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4858, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5385, grad_fn=<MseLossBackward0>)\n",
      "tensor(476.8307, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2724, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3396, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1301, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8238, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0393, grad_fn=<MseLossBackward0>)\n",
      "tensor(333.9702, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.6092, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0094, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0839, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8775, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2464, grad_fn=<MseLossBackward0>)\n",
      "tensor(14.3057, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8253, grad_fn=<MseLossBackward0>)\n",
      "tensor(13.2140, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.7760, grad_fn=<MseLossBackward0>)\n",
      "tensor(17.5254, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "learning_rate = 0.001\n",
    "discount_factor = 0.99\n",
    "initial_epsilon = 0.99\n",
    "min_exploration_prob = 0.01\n",
    "decay = 25\n",
    "\n",
    "epochs = 5000\n",
    "\n",
    "replay_buffer_batch_size = 64\n",
    "min_replay_buffer_size = 100000\n",
    "replay_buffer = deque(maxlen=min_replay_buffer_size)\n",
    "\n",
    "target_network = Model()\n",
    "target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_network.parameters(), learning_rate)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "rewards = []\n",
    "\n",
    "losses = []\n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "loss = -100\n",
    "\n",
    "for i in range(epochs) :\n",
    "\n",
    "\n",
    "    terminal = False\n",
    "\n",
    "    if i % 10 == 0 :\n",
    "        target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "    current_state = env.reset()\n",
    "\n",
    "    rewardsum = 0\n",
    "\n",
    "    p = False\n",
    "\n",
    "    while not terminal :\n",
    "\n",
    "        #env.render()\n",
    "\n",
    "        exploration_prob = epsilon_decay(initial_epsilon, min_exploration_prob, total_steps, decay)\n",
    "\n",
    "\n",
    "        if np.random.rand() < exploration_prob:\n",
    "            action = env.action_space.sample()  \n",
    "        else:\n",
    "            state_tensor = torch.tensor(np.array([current_state]), dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_network(state_tensor)\n",
    "            action = torch.argmax(q_values).item()\n",
    "        \n",
    "        total_steps+=1\n",
    "        next_state, reward, terminal, info = env.step(action)\n",
    "\n",
    "        \n",
    "\n",
    "        rewardsum+=reward\n",
    "\n",
    "        replay_buffer.append((current_state, action, terminal, reward, next_state))\n",
    "\n",
    "        if(len(replay_buffer) >= min_replay_buffer_size) :\n",
    "\n",
    "            minibatch = random.sample(replay_buffer, replay_buffer_batch_size)\n",
    "\n",
    "            batch_states = torch.tensor([transition[0] for transition in minibatch], dtype=torch.float32)\n",
    "            batch_actions = torch.tensor([transition[1] for transition in minibatch], dtype=torch.int64)\n",
    "            batch_terminal = torch.tensor([transition[2] for transition in minibatch], dtype=torch.bool)\n",
    "            batch_rewards = torch.tensor([transition[3] for transition in minibatch], dtype=torch.float32)\n",
    "            batch_next_states = torch.tensor([transition[4] for transition in minibatch], dtype=torch.float32)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                q_values_next = target_network(batch_next_states).detach()\n",
    "                max_q_values_next = q_values_next.max(1)[0] \n",
    "\n",
    "            y = batch_rewards + (discount_factor * max_q_values_next * (~batch_terminal))\n",
    "\n",
    "            q_values = policy_network(batch_states).gather(1, batch_actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "            loss = loss_function(y,q_values)\n",
    "\n",
    "            losses.append(loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(policy_network.parameters(), 10)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        if i%100 == 0 and not p:\n",
    "            print(loss)\n",
    "            p = True\n",
    "\n",
    "        current_state = next_state\n",
    "        \n",
    "        \n",
    "\n",
    "    rewards.append(rewardsum)\n",
    "\n",
    "torch.save(policy_network, 'lunar_game.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#plt.plot(range(len(losses)), [l.detach().numpy() for l in losses])\n",
    "\n",
    "#print(np.min(losses), axis = 1)\n",
    "\n",
    "#print(losses[len(losses)-1])\n",
    "\n",
    "torch.save(policy_network, 'lunar_game.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276.57535738763823\n",
      "267.6144901336594\n",
      "295.5353354565924\n",
      "303.39679811709243\n",
      "310.0734047431989\n",
      "276.80588372281625\n",
      "304.89345745055317\n",
      "294.69889251070197\n",
      "270.6003746131094\n",
      "263.98265514434286\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    episodes = 2000\n",
    "    rewards = []\n",
    "\n",
    "\n",
    "    for episode in range(episodes) :\n",
    "        state = env.reset()\n",
    "        ended = False\n",
    "        rewardsum = 0\n",
    "        #state, reward, ended, info = env.step(env.action_space.sample())\n",
    "        state, reward, ended, info = env.step(env.action_space.sample())\n",
    "        while not ended :\n",
    "            env.render()\n",
    "            #state, reward, ended, info = env.step(env.action_space.sample())\n",
    "            state_tensor = torch.tensor([state], dtype=torch.float32)\n",
    "            q_values = policy_network(state_tensor)\n",
    "            action = torch.argmax(q_values).item()\n",
    "\n",
    "            state, reward, ended, info = env.step(action)\n",
    "            rewardsum += reward\n",
    "        print(rewardsum)\n",
    "        t.sleep(1)\n",
    "        rewards.append(rewardsum)\n",
    "\n",
    "print(np.max(rewards))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
