{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym\n",
    "from IPython import display\n",
    "import time as t\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 2000\n",
    "observations = []\n",
    "rewards = []\n",
    "\n",
    "\n",
    "for episode in range(episodes) :\n",
    "    initial_state = env.reset()\n",
    "    ended = False\n",
    "    rewardsum = 0\n",
    "    while not ended :\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, ended, info = env.step(action)\n",
    "        #print(env.observation_space)\n",
    "        observations.append(obs)\n",
    "        rewardsum += reward\n",
    "        #print(rewardsum)\n",
    "    t.sleep(5)\n",
    "\n",
    "velocities = []\n",
    "ang_velocities = []\n",
    "for observation in observations :\n",
    "    velocities.append(observation[1])\n",
    "    ang_velocities.append(observation[3])\n",
    "\n",
    "print (f'min velocity : {np.min(velocities)}, max velocity : {np.max(velocities)}\\n')\n",
    "print (f'min ang_velocity : {np.min(ang_velocities)}, max ang_velocity : {np.max(ang_velocities)}')\n",
    "      \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "\n",
    "\n",
    "for episode in range(episodes) :\n",
    "    initial_state = env.reset()\n",
    "    ended = False\n",
    "    rewardsum = 0\n",
    "    while not ended :\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, ended, info = env.step(action)\n",
    "        #print(env.observation_space)\n",
    "        observations.append(obs)\n",
    "        rewardsum += reward\n",
    "        #print(rewardsum)\n",
    "    t.sleep(5)\n",
    "\n",
    "velocities = []\n",
    "ang_velocities = []\n",
    "for observation in observations :\n",
    "    velocities.append(observation[1])\n",
    "    ang_velocities.append(observation[3])\n",
    "\n",
    "print (f'min velocity : {np.min(velocities)}, max velocity : {np.max(velocities)}\\n')\n",
    "print (f'min ang_velocity : {np.min(ang_velocities)}, max ang_velocity : {np.max(ang_velocities)}')\n",
    "      \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapStateSpace(obs):\n",
    "\n",
    "    max_position = 2.4 \n",
    "    max_velocity = 5   \n",
    "    max_angle = np.pi   \n",
    "    max_angular_velocity = 4  \n",
    "\n",
    "    # Normalize and discretize \n",
    "    a = np.round((obs[0] / max_position) * 10)  # Discretize position into 10 bins\n",
    "    b = np.round((obs[1] / max_velocity) * 15)  # Discretize velocity into 10 bins\n",
    "    c = np.round((obs[2] / max_angle) * 36)     # Discretize angle into 18 bins (~20 degrees each)\n",
    "    d = np.round((obs[3] / max_angular_velocity) * 15)  # Discretize angular velocity into 10 bins\n",
    "\n",
    "    return int(10*a + 10*b + 30*c + 10*d)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_states = 4000  \n",
    "n_actions = 2\n",
    "\n",
    "Q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_factor = 1\n",
    "exploration_prob = 0.2\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    current_state = env.reset()\n",
    "    #env.render()\n",
    "    ended = False\n",
    "    rewardsum = 0\n",
    "\n",
    "\n",
    "    while not ended :\n",
    "\n",
    "        if np.random.rand() < exploration_prob:\n",
    "            action = env.action_space.sample()  \n",
    "        else:\n",
    "            action = np.argmax(Q_table[mapStateSpace(current_state)])  \n",
    "        \n",
    "        next_state, reward, ended, info = env.step(action)\n",
    "\n",
    "        next_max = np.max(Q_table[mapStateSpace(next_state)])\n",
    "        td_target = reward + discount_factor * next_max\n",
    "        td_error = td_target - Q_table[mapStateSpace(current_state), action]\n",
    "        Q_table[mapStateSpace(current_state), action] += learning_rate * td_error\n",
    "        rewardsum += reward\n",
    "\n",
    "        current_state = next_state \n",
    "    #print(rewardsum)\n",
    "    \n",
    "\n",
    "print(\"Learned Q-table:\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 2000\n",
    "rewards = []\n",
    "\n",
    "\n",
    "for episode in range(episodes) :\n",
    "    state = env.reset()\n",
    "    ended = False\n",
    "    rewardsum = 0\n",
    "    while not ended :\n",
    "        env.render()\n",
    "        action = np.argmax(Q_table[mapStateSpace(state)])\n",
    "        state, reward, ended, info = env.step(action)\n",
    "        rewardsum += reward\n",
    "    #print(rewardsum)\n",
    "    #t.sleep(1)\n",
    "    rewards.append(rewardsum)\n",
    "\n",
    "print(np.max(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(rewards)), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features=4, h1=64, h2=32, out_features=2) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features,h1)\n",
    "        self.fc2 = nn.Linear(h1,h2)\n",
    "        self.out = nn.Linear(h2, out_features)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.dropout(x, 0.2)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "policy_network = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def epsilon_decay(epsilon, t, epsilon_min, epsilon_decay):\n",
    "    return max(epsilon_min, min(epsilon, 1.0 - math.log10((t+1) * epsilon_decay)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "discount_factor = 0.9\n",
    "exploration_prob = 1.0\n",
    "min_exploration_prob = 0.01\n",
    "decay = 0.999\n",
    "\n",
    "epochs = 5000\n",
    "\n",
    "replay_buffer_batch_size = 128\n",
    "min_replay_buffer_size = 5000\n",
    "replay_buffer = deque(maxlen=min_replay_buffer_size)\n",
    "\n",
    "target_network = Model()\n",
    "target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_network.parameters(), learning_rate)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "rewards = []\n",
    "\n",
    "losses = []\n",
    "\n",
    "loss = -100\n",
    "\n",
    "for i in range(epochs) :\n",
    "\n",
    "    #print(exploration_prob)\n",
    "    exploration_prob = epsilon_decay(exploration_prob, i, min_exploration_prob, decay)\n",
    "\n",
    "    terminal = False\n",
    "\n",
    "    if i % 30 == 0 :\n",
    "        target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "    current_state = env.reset()\n",
    "\n",
    "    rewardsum = 0\n",
    "\n",
    "    p = False\n",
    "\n",
    "    while not terminal :\n",
    "\n",
    "        #env.render()\n",
    "\n",
    "        if np.random.rand() < exploration_prob:\n",
    "            action = env.action_space.sample()  \n",
    "        else:\n",
    "            state_tensor = torch.tensor(np.array([current_state]), dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_network(state_tensor)\n",
    "            action = torch.argmax(q_values).item()\n",
    "        \n",
    "        next_state, reward, terminal, info = env.step(action)\n",
    "\n",
    "        rewardsum+=reward\n",
    "\n",
    "        replay_buffer.append((current_state, action, terminal, reward, next_state))\n",
    "\n",
    "        if(len(replay_buffer) >= min_replay_buffer_size) :\n",
    "\n",
    "            minibatch = random.sample(replay_buffer, replay_buffer_batch_size)\n",
    "\n",
    "            batch_states = torch.tensor([transition[0] for transition in minibatch], dtype=torch.float32)\n",
    "            batch_actions = torch.tensor([transition[1] for transition in minibatch], dtype=torch.int64)\n",
    "            batch_terminal = torch.tensor([transition[2] for transition in minibatch], dtype=torch.bool)\n",
    "            batch_rewards = torch.tensor([transition[3] for transition in minibatch], dtype=torch.float32)\n",
    "            batch_next_states = torch.tensor([transition[4] for transition in minibatch], dtype=torch.float32)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                q_values_next = target_network(batch_next_states).detach()\n",
    "                max_q_values_next = q_values_next.max(1)[0] \n",
    "\n",
    "            y = batch_rewards + (discount_factor * max_q_values_next * (~batch_terminal))    \n",
    "\n",
    "            q_values = policy_network(batch_states).gather(1, batch_actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "            loss = loss_function(y,q_values)\n",
    "\n",
    "            losses.append(loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            #torch.nn.utils.clip_grad_norm_(policy_network.parameters(), 10)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        if i%100 == 0 and not p:\n",
    "            print(loss)\n",
    "            p = True\n",
    "\n",
    "        current_state = next_state\n",
    "        \n",
    "        \n",
    "\n",
    "    rewards.append(rewardsum)\n",
    "\n",
    "    torch.save(policy_network, 'stick_game.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plt.plot(range(len(losses)), [l.detach().numpy() for l in losses])\n",
    "\n",
    "#print(losses[len(losses)-1])\n",
    "\n",
    "torch.save(policy_network, 'stick_game.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n",
      "499.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#state, reward, ended, info = env.step(env.action_space.sample())\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ended :\n\u001b[0;32m---> 13\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#state, reward, ended, info = env.step(env.action_space.sample())\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([state], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gym/core.py:343\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Renders the environment with kwargs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gym/wrappers/env_checker.py:57\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m passive_env_render_check(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gym/envs/classic_control/cartpole.py:273\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    272\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    episodes = 2000\n",
    "    rewards = []\n",
    "\n",
    "\n",
    "    for episode in range(episodes) :\n",
    "        state = env.reset()\n",
    "        ended = False\n",
    "        rewardsum = 0\n",
    "        #state, reward, ended, info = env.step(env.action_space.sample())\n",
    "        #state, reward, ended, info = env.step(env.action_space.sample())\n",
    "        while not ended :\n",
    "            env.render()\n",
    "            #state, reward, ended, info = env.step(env.action_space.sample())\n",
    "            state_tensor = torch.tensor([state], dtype=torch.float32)\n",
    "            q_values = policy_network(state_tensor)\n",
    "            action = torch.argmax(q_values).item()\n",
    "\n",
    "            state, reward, ended, info = env.step(action)\n",
    "            rewardsum += reward\n",
    "        print(rewardsum)\n",
    "        #t.sleep(1)\n",
    "        rewards.append(rewardsum)\n",
    "\n",
    "print(np.max(rewards))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
