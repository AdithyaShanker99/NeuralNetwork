{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
      "/Users/adithyashanker/anaconda3/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym\n",
    "from IPython import display\n",
    "import time as t\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "observations = []\n",
    "rewards = []\n",
    "\n",
    "\n",
    "for episode in range(episodes) :\n",
    "    initial_state = env.reset()\n",
    "    ended = False\n",
    "    rewardsum = 0\n",
    "    while not ended :\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, ended, info = env.step(action)\n",
    "        #print(env.observation_space)\n",
    "        observations.append(obs)\n",
    "        rewardsum += reward\n",
    "        #print(rewardsum)\n",
    "    #t.sleep(5)\n",
    "\n",
    "velocities = []\n",
    "ang_velocities = []\n",
    "for observation in observations :\n",
    "    velocities.append(observation[1])\n",
    "    ang_velocities.append(observation[3])\n",
    "\n",
    "print (f'min velocity : {np.min(velocities)}, max velocity : {np.max(velocities)}\\n')\n",
    "print (f'min ang_velocity : {np.min(ang_velocities)}, max ang_velocity : {np.max(ang_velocities)}')\n",
    "      \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "\n",
    "\n",
    "for episode in range(episodes) :\n",
    "    initial_state = env.reset()\n",
    "    ended = False\n",
    "    rewardsum = 0\n",
    "    while not ended :\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, ended, info = env.step(action)\n",
    "        #print(env.observation_space)\n",
    "        observations.append(obs)\n",
    "        rewardsum += reward\n",
    "        #print(rewardsum)\n",
    "    t.sleep(5)\n",
    "\n",
    "velocities = []\n",
    "ang_velocities = []\n",
    "for observation in observations :\n",
    "    velocities.append(observation[1])\n",
    "    ang_velocities.append(observation[3])\n",
    "\n",
    "print (f'min velocity : {np.min(velocities)}, max velocity : {np.max(velocities)}\\n')\n",
    "print (f'min ang_velocity : {np.min(ang_velocities)}, max ang_velocity : {np.max(ang_velocities)}')\n",
    "      \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapStateSpace(obs):\n",
    "\n",
    "    max_position = 2.4 \n",
    "    max_velocity = 5   \n",
    "    max_angle = np.pi   \n",
    "    max_angular_velocity = 4  \n",
    "\n",
    "    # Normalize and discretize \n",
    "    a = np.round((obs[0] / max_position) * 10)  # Discretize position into 10 bins\n",
    "    b = np.round((obs[1] / max_velocity) * 15)  # Discretize velocity into 10 bins\n",
    "    c = np.round((obs[2] / max_angle) * 36)     # Discretize angle into 18 bins (~20 degrees each)\n",
    "    d = np.round((obs[3] / max_angular_velocity) * 15)  # Discretize angular velocity into 10 bins\n",
    "\n",
    "    return int(10*a + 10*b + 30*c + 10*d)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_states = 4000  \n",
    "n_actions = 2\n",
    "\n",
    "Q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_factor = 1\n",
    "exploration_prob = 0.2\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    current_state = env.reset()\n",
    "    #env.render()\n",
    "    ended = False\n",
    "    rewardsum = 0\n",
    "\n",
    "\n",
    "    while not ended :\n",
    "\n",
    "        if np.random.rand() < exploration_prob:\n",
    "            action = env.action_space.sample()  \n",
    "        else:\n",
    "            action = np.argmax(Q_table[mapStateSpace(current_state)])  \n",
    "        \n",
    "        next_state, reward, ended, info = env.step(action)\n",
    "\n",
    "        next_max = np.max(Q_table[mapStateSpace(next_state)])\n",
    "        td_target = reward + discount_factor * next_max\n",
    "        td_error = td_target - Q_table[mapStateSpace(current_state), action]\n",
    "        Q_table[mapStateSpace(current_state), action] += learning_rate * td_error\n",
    "        rewardsum += reward\n",
    "\n",
    "        current_state = next_state \n",
    "    #print(rewardsum)\n",
    "    \n",
    "\n",
    "print(\"Learned Q-table:\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 2000\n",
    "rewards = []\n",
    "\n",
    "\n",
    "for episode in range(episodes) :\n",
    "    state = env.reset()\n",
    "    ended = False\n",
    "    rewardsum = 0\n",
    "    while not ended :\n",
    "        env.render()\n",
    "        action = np.argmax(Q_table[mapStateSpace(state)])\n",
    "        state, reward, ended, info = env.step(action)\n",
    "        rewardsum += reward\n",
    "    #print(rewardsum)\n",
    "    #t.sleep(1)\n",
    "    rewards.append(rewardsum)\n",
    "\n",
    "print(np.max(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(rewards)), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features=8, h1=64, h2=64, h3=64, out_features=4) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features,h1)\n",
    "        self.fc2 = nn.Linear(h1,h2)\n",
    "        self.fc3 = nn.Linear(h2, h3)\n",
    "        self.out = nn.Linear(h3, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.dropout(x, 0.2)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x = F.dropout(x, 0.2)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "policy_network = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def epsilon_decay(epsilon_start, epsilon_final, steps, epsilon_decay):\n",
    "    epsilon = epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * steps / epsilon_decay)\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sy/2jzm18gs78s48trcqgthb1xh0000gn/T/ipykernel_62032/107616033.py:73: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  batch_states = torch.tensor([transition[0] for transition in minibatch], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(79.3885, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9302, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5425, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6607, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0427, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7811, grad_fn=<MseLossBackward0>)\n",
      "tensor(12.0813, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7673, grad_fn=<MseLossBackward0>)\n",
      "tensor(286.3869, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7295, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.7655, grad_fn=<MseLossBackward0>)\n",
      "tensor(27.8934, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4923, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6464, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8472, grad_fn=<MseLossBackward0>)\n",
      "tensor(47.9638, grad_fn=<MseLossBackward0>)\n",
      "tensor(163.0980, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6291, grad_fn=<MseLossBackward0>)\n",
      "tensor(441.7270, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4858, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5385, grad_fn=<MseLossBackward0>)\n",
      "tensor(476.8307, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2724, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3396, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1301, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8238, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0393, grad_fn=<MseLossBackward0>)\n",
      "tensor(333.9702, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.6092, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0094, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0839, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8775, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2464, grad_fn=<MseLossBackward0>)\n",
      "tensor(14.3057, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8253, grad_fn=<MseLossBackward0>)\n",
      "tensor(13.2140, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.7760, grad_fn=<MseLossBackward0>)\n",
      "tensor(17.5254, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "learning_rate = 0.001\n",
    "discount_factor = 0.99\n",
    "initial_epsilon = 0.99\n",
    "min_exploration_prob = 0.01\n",
    "decay = 25\n",
    "\n",
    "epochs = 5000\n",
    "\n",
    "replay_buffer_batch_size = 64\n",
    "min_replay_buffer_size = 100000\n",
    "replay_buffer = deque(maxlen=min_replay_buffer_size)\n",
    "\n",
    "target_network = Model()\n",
    "target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_network.parameters(), learning_rate)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "rewards = []\n",
    "\n",
    "losses = []\n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "loss = -100\n",
    "\n",
    "for i in range(epochs) :\n",
    "\n",
    "\n",
    "    terminal = False\n",
    "\n",
    "    if i % 10 == 0 :\n",
    "        target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "    current_state = env.reset()\n",
    "\n",
    "    rewardsum = 0\n",
    "\n",
    "    p = False\n",
    "\n",
    "    while not terminal :\n",
    "\n",
    "        #env.render()\n",
    "\n",
    "        exploration_prob = epsilon_decay(initial_epsilon, min_exploration_prob, total_steps, decay)\n",
    "\n",
    "\n",
    "        if np.random.rand() < exploration_prob:\n",
    "            action = env.action_space.sample()  \n",
    "        else:\n",
    "            state_tensor = torch.tensor(np.array([current_state]), dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_network(state_tensor)\n",
    "            action = torch.argmax(q_values).item()\n",
    "        \n",
    "        total_steps+=1\n",
    "        next_state, reward, terminal, info = env.step(action)\n",
    "\n",
    "        \n",
    "\n",
    "        rewardsum+=reward\n",
    "\n",
    "        replay_buffer.append((current_state, action, terminal, reward, next_state))\n",
    "\n",
    "        if(len(replay_buffer) >= min_replay_buffer_size) :\n",
    "\n",
    "            minibatch = random.sample(replay_buffer, replay_buffer_batch_size)\n",
    "\n",
    "            batch_states = torch.tensor([transition[0] for transition in minibatch], dtype=torch.float32)\n",
    "            batch_actions = torch.tensor([transition[1] for transition in minibatch], dtype=torch.int64)\n",
    "            batch_terminal = torch.tensor([transition[2] for transition in minibatch], dtype=torch.bool)\n",
    "            batch_rewards = torch.tensor([transition[3] for transition in minibatch], dtype=torch.float32)\n",
    "            batch_next_states = torch.tensor([transition[4] for transition in minibatch], dtype=torch.float32)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                q_values_next = target_network(batch_next_states).detach()\n",
    "                max_q_values_next = q_values_next.max(1)[0] \n",
    "\n",
    "            y = batch_rewards + (discount_factor * max_q_values_next * (~batch_terminal))\n",
    "\n",
    "            q_values = policy_network(batch_states).gather(1, batch_actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "            loss = loss_function(y,q_values)\n",
    "\n",
    "            losses.append(loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(policy_network.parameters(), 10)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        if i%100 == 0 and not p:\n",
    "            print(loss)\n",
    "            p = True\n",
    "\n",
    "        current_state = next_state\n",
    "        \n",
    "        \n",
    "\n",
    "    rewards.append(rewardsum)\n",
    "\n",
    "torch.save(policy_network, 'lunar_game.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5648, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAGvCAYAAABSC3+tAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA3h0lEQVR4nO3deXhU9d3//1dISAiRTFlMQgQRLVIwQDVUNi24EFSWequ3Xne881O/ilqUpUIt1Nalt4KVxaW4ULXgAsQqoFYwJlZlXwNRQthkTUjCmkxCyDrz+f1BGR2CkAmZnJmT5+O65rqac94z530+jZxXPnOWEGOMEQAAgA01s7oBAAAAfyHoAAAA2yLoAAAA2yLoAAAA2yLoAAAA2yLoAAAA2yLoAAAA2yLoAAAA2wqzugErud1u5efnq1WrVgoJCbG6HQAAUAfGGJWWlio+Pl7Nmp19zqZJB538/Hx17NjR6jYAAEA95ObmqkOHDmetadJBp1WrVpJODlR0dLTF3QAAgLooKSlRx44dPcfxs2nSQefU11XR0dEEHQAAgkxdTjvhZGQAAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AsIkvthRq0sLNqqpxW90KEDCa9NPLAcBOHnovU5J0eewFum9AZ4u7AQIDMzoAYDOHSiutbgEIGAQdAABgWwQdAABgWwQdAABgWwQdAABgWwQdAABgWwQdAABgWwQdAABgWwQdAABgWwQdAABgWwQdAABgWwQdAABgWwQdAABgWwQdAABgWwQdAABgWwQdAABgW+cVdKZMmaKQkBCNGzfOs8wYo6efflrx8fGKjIzUoEGDtGXLFq/3VVZWavTo0WrXrp2ioqI0YsQI5eXledUUFRUpJSVFDodDDodDKSkpKi4u9qrZv3+/hg8frqioKLVr105jxoxRVVXV+ewSAAQ9Y6zuAAgc9Q4669ev19///nf17NnTa/kLL7ygGTNmaObMmVq/fr3i4uI0ePBglZaWemrGjRunRYsWKTU1VStWrNDx48c1bNgwuVwuT01ycrKysrKUlpamtLQ0ZWVlKSUlxbPe5XJp6NChKisr04oVK5SamqoFCxZo/Pjx9d0lAABgN6YeSktLTZcuXUxGRoYZOHCgGTt2rDHGGLfbbeLi4szzzz/vqa2oqDAOh8O88cYbxhhjiouLTfPmzU1qaqqn5sCBA6ZZs2YmLS3NGGNMTk6OkWTWrFnjqVm9erWRZLZt22aMMWbJkiWmWbNm5sCBA56a+fPnm4iICON0Ouu0H06n00iqcz0ABLJOf/jMdPrDZ2bKkq1WtwL4lS/H73rN6DzyyCMaOnSobrzxRq/le/bsUWFhoZKSkjzLIiIiNHDgQK1atUqSlJmZqerqaq+a+Ph4JSQkeGpWr14th8OhPn36eGr69u0rh8PhVZOQkKD4+HhPzZAhQ1RZWanMzMwz9l1ZWamSkhKvFwAAsK8wX9+QmpqqjRs3av369bXWFRYWSpJiY2O9lsfGxmrfvn2emvDwcLVu3bpWzan3FxYWKiYmptbnx8TEeNWcvp3WrVsrPDzcU3O6KVOm6JlnnqnLbgIAABvwaUYnNzdXY8eO1fvvv68WLVr8ZF1ISIjXz8aYWstOd3rNmerrU/NjkyZNktPp9Lxyc3PP2hMAAAhuPgWdzMxMHTp0SImJiQoLC1NYWJiWLl2qV155RWFhYZ4ZltNnVA4dOuRZFxcXp6qqKhUVFZ215uDBg7W2f/jwYa+a07dTVFSk6urqWjM9p0RERCg6OtrrBQAA7MunoHPDDTdo8+bNysrK8rx69+6tu+++W1lZWbr00ksVFxenjIwMz3uqqqq0dOlS9e/fX5KUmJio5s2be9UUFBQoOzvbU9OvXz85nU6tW7fOU7N27Vo5nU6vmuzsbBUUFHhq0tPTFRERocTExHoMBQAAsBufztFp1aqVEhISvJZFRUWpbdu2nuXjxo3T5MmT1aVLF3Xp0kWTJ09Wy5YtlZycLElyOBy6//77NX78eLVt21Zt2rTRhAkT1KNHD8/Jzd26ddNNN92kkSNHatasWZKkBx98UMOGDVPXrl0lSUlJSerevbtSUlI0depUHTt2TBMmTNDIkSOZqQEAAJLqcTLyuTz++OMqLy/XqFGjVFRUpD59+ig9PV2tWrXy1Lz44osKCwvTnXfeqfLyct1www2aM2eOQkNDPTVz587VmDFjPFdnjRgxQjNnzvSsDw0N1eLFizVq1CgNGDBAkZGRSk5O1rRp0xp6lwAAQJAKMabp3kOzpKREDodDTqeTWSAAQe+SiYslSQ8PvEwTb/6Fxd0A/uPL8ZtnXQEAANsi6AAAANsi6AAAANsi6AAAANsi6AAAANsi6ACAzRg12YtpgVoIOgAAwLYIOgAAwLYIOgAAwLYIOgAAwLYIOgAAwLYIOgAAwLYIOgAAwLYIOgAAwLYIOgAAwLYIOgAAwLYIOgAAwLYIOgAAwLYIOgAAwLYIOgAAwLYIOgAAwLYIOgBgMyEKsboFIGAQdADAZoyM1S0AAYOgAwAAbIugAwAAbIugAwAAbIugAwAAbIugAwAAbIugAwAAbIugAwAAbIugAwAAbIugAwAAbIugAwAAbIugAwAAbIugAwAAbIugAwAAbIugAwAAbIugAwAAbIugAwAAbIugAwB2Y6xuAAgcBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AsBljdQNAACHoAAAA2yLoAAAA2yLoAAAA2yLoAAAA2yLoAAAA2yLoAAAA2yLoAAAA2yLoAAAA2yLoAICN1bjc2l5YKmO4jSCaJoIOANjY2A+yNOSlZfrHyr1WtwJYgqADADa2+LsCSdKspbss7gSwhk9B5/XXX1fPnj0VHR2t6Oho9evXT59//rlnvTFGTz/9tOLj4xUZGalBgwZpy5YtXp9RWVmp0aNHq127doqKitKIESOUl5fnVVNUVKSUlBQ5HA45HA6lpKSouLjYq2b//v0aPny4oqKi1K5dO40ZM0ZVVVU+7j4AALAzn4JOhw4d9Pzzz2vDhg3asGGDrr/+ev3mN7/xhJkXXnhBM2bM0MyZM7V+/XrFxcVp8ODBKi0t9XzGuHHjtGjRIqWmpmrFihU6fvy4hg0bJpfL5alJTk5WVlaW0tLSlJaWpqysLKWkpHjWu1wuDR06VGVlZVqxYoVSU1O1YMECjR8//nzHAwCCXojVDQCBxJyn1q1bm7feesu43W4TFxdnnn/+ec+6iooK43A4zBtvvGGMMaa4uNg0b97cpKamemoOHDhgmjVrZtLS0owxxuTk5BhJZs2aNZ6a1atXG0lm27ZtxhhjlixZYpo1a2YOHDjgqZk/f76JiIgwTqezzr07nU4jyaf3AECg6vSHz0ynP3xmJi/OqbWs0x8+M6nr9lnYHdBwfDl+1/scHZfLpdTUVJWVlalfv37as2ePCgsLlZSU5KmJiIjQwIEDtWrVKklSZmamqqurvWri4+OVkJDgqVm9erUcDof69Onjqenbt68cDodXTUJCguLj4z01Q4YMUWVlpTIzM3+y58rKSpWUlHi9AKCp+MOCzVa3ADQ6n4PO5s2bdcEFFygiIkIPP/ywFi1apO7du6uwsFCSFBsb61UfGxvrWVdYWKjw8HC1bt36rDUxMTG1thsTE+NVc/p2WrdurfDwcE/NmUyZMsVz3o/D4VDHjh193HsAABBMfA46Xbt2VVZWltasWaPf/va3uueee5STk+NZHxLi/e2wMabWstOdXnOm+vrUnG7SpElyOp2eV25u7ln7AoBgxB1zgB/4HHTCw8P185//XL1799aUKVPUq1cvvfzyy4qLi5OkWjMqhw4d8sy+xMXFqaqqSkVFRWetOXjwYK3tHj582Kvm9O0UFRWpurq61kzPj0VERHiuGDv1AgAA9nXe99ExxqiyslKdO3dWXFycMjIyPOuqqqq0dOlS9e/fX5KUmJio5s2be9UUFBQoOzvbU9OvXz85nU6tW7fOU7N27Vo5nU6vmuzsbBUUFHhq0tPTFRERocTExPPdJQAAYBNhvhT/8Y9/1M0336yOHTuqtLRUqamp+uabb5SWlqaQkBCNGzdOkydPVpcuXdSlSxdNnjxZLVu2VHJysiTJ4XDo/vvv1/jx49W2bVu1adNGEyZMUI8ePXTjjTdKkrp166abbrpJI0eO1KxZsyRJDz74oIYNG6auXbtKkpKSktS9e3elpKRo6tSpOnbsmCZMmKCRI0cySwMAADx8CjoHDx5USkqKCgoK5HA41LNnT6WlpWnw4MGSpMcff1zl5eUaNWqUioqK1KdPH6Wnp6tVq1aez3jxxRcVFhamO++8U+Xl5brhhhs0Z84chYaGemrmzp2rMWPGeK7OGjFihGbOnOlZHxoaqsWLF2vUqFEaMGCAIiMjlZycrGnTpp3XYAAAAHsJMabpPumtpKREDodDTqeTmSAAQe+SiYslSQ/++lL98ZZuXstO2fv80EbvC2hovhy/edYVAACwLYIOAACwLYIOAACwLYIOANjUpv1F5y4CbI6gAwA2tHBjnv7rtVVWtwFYjqADADY08+vvrW4BCAgEHQCwmWqXW7sPl1ndBhAQCDoAYDOzV+61ugUgYBB0AACAbRF0AACAbRF0AACAbRF0AACAbRF0AACAbRF0AACAbRF0AACAbRF0AACAbRF0AACAbRF0AACAbRF0AACAbRF0AACAbRF0AACAbRF0AACAbRF0AACAbRF0AACAbRF0AACAbRF0AMAGKqpdVrcABCSCDgDYwPXTvrG6BSAgEXQAwAbynRVWtwAEJIIOAAS5L7YUWt0CELAIOgAQ5B56L9PqFoCARdABAAC2RdABAAC2RdABAAC2RdABAAC2RdABAAC2RdABAAC2RdABAAC2RdABAAC2RdABAAC2RdABAAC2RdABAAC2RdABAAC2RdABAAC2RdABAAC2RdABAAC2RdABgCDkPFFtdQtAUCDoAECQ+SgzT73+kq6Xv9xpdStAwCPoAECQ+ePCzZKkF7/cYXEnvnO7jQqc5Va3gSaEoAMAQSJzX5FGz9+kKpfb6lbq7dH5G9VvyldKyy60uhU0EWFWNwAAqJvbX19ldQvnbcnmkwHn78t26aaEOIu7QVPAjA4AALAtgg4AALAtgg4ABIGaID4vB7ASQQcAgsCby/dY3QIQlAg6ABAgDpZUyOU2qna5tWzHYZVV1kiSKmtc+mvaNou7A4ITV10BQABY9f0RJb+1Vr++/EJ1bx+tN5bu0jU/b6f3H+ijrQWlP/m+B95Z34hdAsGHoAMAAWD2qr2SpGU7Ditrf5EkacX3R875vi+3HvJnW0DQ46srAABgWwQdAAhgh0srrW4BCGoEHQAIYA++t8HqFoCgRtABgAC2aX+x1S0AQY2gAwBoFBXVLqtbQBNE0AGAAFNSUeP1szHGok4a1h8Xbba6BTRBBB0AQKNYuPGA1S2gCSLoAAAaXUhIiNUtoIkg6ABAAGisb6emfbFduw4fb5yNAQGAoAMATcjMr7/XzS8vt7oNoNEQdACgiamqcVvdQp1xpRbOl09BZ8qUKfrVr36lVq1aKSYmRrfeequ2b9/uVWOM0dNPP634+HhFRkZq0KBB2rJli1dNZWWlRo8erXbt2ikqKkojRoxQXl6eV01RUZFSUlLkcDjkcDiUkpKi4uJir5r9+/dr+PDhioqKUrt27TRmzBhVVVX5sksAgAD1t3/v1C/+nKZvtvM8L9SfT0Fn6dKleuSRR7RmzRplZGSopqZGSUlJKisr89S88MILmjFjhmbOnKn169crLi5OgwcPVmnpD0/fHTdunBYtWqTU1FStWLFCx48f17Bhw+Ry/ZDck5OTlZWVpbS0NKWlpSkrK0spKSme9S6XS0OHDlVZWZlWrFih1NRULViwQOPHjz+f8QCAgJOWXWh1C5aYnrFDkvTEomyLO0Ew8+np5WlpaV4/z549WzExMcrMzNSvf/1rGWP00ksv6YknntBtt90mSXrnnXcUGxurefPm6aGHHpLT6dTbb7+t9957TzfeeKMk6f3331fHjh315ZdfasiQIdq6davS0tK0Zs0a9enTR5L05ptvql+/ftq+fbu6du2q9PR05eTkKDc3V/Hx8ZKk6dOn695779Vzzz2n6Ojo8x4cAAgEs5bttroFIGid1zk6TqdTktSmTRtJ0p49e1RYWKikpCRPTUREhAYOHKhVq1ZJkjIzM1VdXe1VEx8fr4SEBE/N6tWr5XA4PCFHkvr27SuHw+FVk5CQ4Ak5kjRkyBBVVlYqMzPzjP1WVlaqpKTE6wUAAOyr3kHHGKPHHntM11xzjRISEiRJhYUnp1djY2O9amNjYz3rCgsLFR4ertatW5+1JiYmptY2Y2JivGpO307r1q0VHh7uqTndlClTPOf8OBwOdezY0dfdBgC/yCs6YXULgC3VO+g8+uij+u677zR//vxa606/EZQx5pw3hzq95kz19an5sUmTJsnpdHpeubm5Z+0JABrLtsLScxcB8Fm9gs7o0aP16aef6uuvv1aHDh08y+Pi4iSp1ozKoUOHPLMvcXFxqqqqUlFR0VlrDh48WGu7hw8f9qo5fTtFRUWqrq6uNdNzSkREhKKjo71eAADAvnwKOsYYPfroo1q4cKG++uorde7c2Wt9586dFRcXp4yMDM+yqqoqLV26VP3795ckJSYmqnnz5l41BQUFys7O9tT069dPTqdT69at89SsXbtWTqfTqyY7O1sFBQWemvT0dEVERCgxMdGX3QKAJudwaaXVLXjZe6RMc1bu4b45aHA+XXX1yCOPaN68efrkk0/UqlUrz4yKw+FQZGSkQkJCNG7cOE2ePFldunRRly5dNHnyZLVs2VLJycme2vvvv1/jx49X27Zt1aZNG02YMEE9evTwXIXVrVs33XTTTRo5cqRmzZolSXrwwQc1bNgwde3aVZKUlJSk7t27KyUlRVOnTtWxY8c0YcIEjRw5kpkaADiHKldg3TRw0LRvJElHjldpwpCu1jYDW/Ep6Lz++uuSpEGDBnktnz17tu69915J0uOPP67y8nKNGjVKRUVF6tOnj9LT09WqVStP/YsvvqiwsDDdeeedKi8v1w033KA5c+YoNDTUUzN37lyNGTPGc3XWiBEjNHPmTM/60NBQLV68WKNGjdKAAQMUGRmp5ORkTZs2zacBAAA0vpqfCFrr9hxr5E5gdyHGNNaj5AJPSUmJHA6HnE4ns0AALHXJxMWNur2VE6/XRT+LbNRtnr6PTw7rrv93Teda6/Y+P9Rr2UU/i9TKidc3UpcIBr4cv3nWFQDAEn/5LMfqFtAEEHQAAIBtEXQAAIBtEXQAoAlqzNMzjTH6Nrf4jOuSXlyqnQe5WSL8h6ADAPCr9JyD+s2rK8+4bsfB4xr84rLTlhF80HAIOgAAv/p8c8G5i34kv7jcT52gKSLoAIDFNuc5rW4hoB0oLldpRbXVbSBIEXQAwGLDZ66wuoWAN2/tfqtbQJAi6AAAAt6K749Y3QKCFEEHABBQSipqai1bvpOgg/oh6AAAAkoTfjIR/ICgAwBNEFkCTQVBBwAQUIpPcIUVGg5BBwAQUJ76dIvVLcBGCDoAAMC2CDoAgICTfYCbKKJhEHQAAAFn2N+4iSIaBkEHAADYFkEHACxU7XJb3QJgawQdALDIiaoa9X72S6vbAGyNoAMAFlm+84ic5dwzBvAngg4AALAtgg4AALAtgg4AICh8lJlndQsIQgQdAEBQmPDht1a3gCBE0AEAi3y+ucDqFgDbI+gAgEU+zsq3ugXA9gg6ANAEGWN1Bw2r2uXWlM+3auX3R6xuBQGGoAMACHpz1+zTrKW7dfdba61uBQGGoAMACHr7j5Vb3QICFEEHAADYFkEHAADYFkEHAADYFkEHACzAwzyBxkHQAQAL/NerKy3dvlHjXV8eEhLSCNvw+yYQpAg6AGCB3UfKrG7BtvKLuQILPyDoAABspf/zXylzX5HVbSBAEHQAALazcCNPOsdJYVY3AACwH5fb6KlPs3XVxa0bZXunn6JzuLSyUbaLwMeMDgCgwaVlF+r9Nfv12D+/tWT76TkHLdkuAg9BBwDQ4I6dqLK6BUASQQcA0MC25DuVtb/Y6jYASZyjAwBNkvHTbXTcbqOhr6zwz4cD9cCMDgCgwdS4G+9GhD/GDQPxUwg6AAC/2lpQYnULaMIIOgAAv9pWWGp1C2jCCDoAgKD35vI9VreAAEXQAQAAtkXQAQA0mMZ4KnrmviKt23PM79uBPRB0AKAJ2ns0OJ+eXlXj1u2vr9Kds1artKLa6nYQBAg6ANDIDpVWWN2C7p293i+f66/785wyNnWT53+XVNT4d2OwBYIOADSyq5/7t9UtBK3PswutbgFBhqADAABsi6ADAAhKz3++TR9l5lndBgIcQQcAEJT+9W2+Jnz4rdVtIMARdACgiXr2sxyrWwD8jqADAE3UWyu4mzDsj6ADAE1YRbXL6hbO22/fz7S6BQQwgg4ANGELNx5o0M/z9310zoRLznE2YVY3AABNRbXLrUWbGjZYnK+qmuCf0QHOhqADAI3knVV79ezirVa34VeN8awrwBd8dQUAjWTN7sB7EKWzvEbXTftGU7/YZnUrgF8QdACgCXtn9V7tOVKmV7/eZXUrgF8QdACgCatxua1uAfArgg4ANGGcUQO7I+gAQBNWWlHToJ9nxeXlPyVzX5EOFJd7fi6rrNGn3+br0XkbbXH/INQNV10BABpMAOUc3f76KknS3ueH6uUvd+rFL3d41nWPj9aoQT+3qjU0ImZ0AKCRhIRY3UHT9eOQI0nHjldZ1Akam89BZ9myZRo+fLji4+MVEhKijz/+2Gu9MUZPP/204uPjFRkZqUGDBmnLli1eNZWVlRo9erTatWunqKgojRgxQnl5eV41RUVFSklJkcPhkMPhUEpKioqLi71q9u/fr+HDhysqKkrt2rXTmDFjVFXFLy8A4Acnqhr26zkEF5+DTllZmXr16qWZM2eecf0LL7ygGTNmaObMmVq/fr3i4uI0ePBglZaWemrGjRunRYsWKTU1VStWrNDx48c1bNgwuVw/fGeanJysrKwspaWlKS0tTVlZWUpJSfGsd7lcGjp0qMrKyrRixQqlpqZqwYIFGj9+vK+7BACwse5PflFr2Y/P3YG9+XyOzs0336ybb775jOuMMXrppZf0xBNP6LbbbpMkvfPOO4qNjdW8efP00EMPyel06u2339Z7772nG2+8UZL0/vvvq2PHjvryyy81ZMgQbd26VWlpaVqzZo369OkjSXrzzTfVr18/bd++XV27dlV6erpycnKUm5ur+Ph4SdL06dN177336rnnnlN0dHS9BgQAYH+fZxdq5fdHNODn7axuBX7WoOfo7NmzR4WFhUpKSvIsi4iI0MCBA7Vq1cmTwjIzM1VdXe1VEx8fr4SEBE/N6tWr5XA4PCFHkvr27SuHw+FVk5CQ4Ak5kjRkyBBVVlYqM/PMT7KtrKxUSUmJ1wsA0DTNX7ff6hbQCBo06BQWnnyCbGxsrNfy2NhYz7rCwkKFh4erdevWZ62JiYmp9fkxMTFeNadvp3Xr1goPD/fUnG7KlCmec34cDoc6duxYj70EgLp56csd+tu/d3p+bgrnIptAur4ckJ+uugo57dICY0ytZac7veZM9fWp+bFJkybJ6XR6Xrm5uWftCQDq6+jxSr305U5Nz9ih45VN52TYYIo5n31XYHULaAQNGnTi4uIkqdaMyqFDhzyzL3FxcaqqqlJRUdFZaw4ePFjr8w8fPuxVc/p2ioqKVF1dXWum55SIiAhFR0d7vQDAH6p+9GgFl/vk4f/LrbX/XQtUbrfR2t1Hm1RIgz01aNDp3Lmz4uLilJGR4VlWVVWlpUuXqn///pKkxMRENW/e3KumoKBA2dnZnpp+/frJ6XRq3bp1npq1a9fK6XR61WRnZ6ug4IdEnp6eroiICCUmJjbkbgFAg3AH+HRHaUW153+/t2af7vr7Gt01a7WFHQHnz+erro4fP67vv//e8/OePXuUlZWlNm3a6OKLL9a4ceM0efJkdenSRV26dNHkyZPVsmVLJScnS5IcDofuv/9+jR8/Xm3btlWbNm00YcIE9ejRw3MVVrdu3XTTTTdp5MiRmjVrliTpwQcf1LBhw9S1a1dJUlJSkrp3766UlBRNnTpVx44d04QJEzRy5EhmagCgHqZ8vk2T/6uHJGnhxpP3NtuSz0UbCG4+B50NGzbouuuu8/z82GOPSZLuuecezZkzR48//rjKy8s1atQoFRUVqU+fPkpPT1erVq0873nxxRcVFhamO++8U+Xl5brhhhs0Z84chYaGemrmzp2rMWPGeK7OGjFihNe9e0JDQ7V48WKNGjVKAwYMUGRkpJKTkzVt2jTfRwEA/GxBZt65iyy25YDT6haABhdimvAp8iUlJXI4HHI6ncwCAWhQ+cXl6v/8V5Kk755OUs+n0y3uqG7+8psrdHGblnoxY4e+zTsZfPY+P7TO73eWV6vXM8Gxr5Jv+4bA4cvxm4d6AoAf/PgvyGC6rPzJT04+sufnMRdY3AnQMHioJwCglvKqHx7JU+DkcQkIXgQdAPCzkorgu0T7x8+C6jflK/1zQx3vO9ZkT4ZAoCLoAICfDX1ludUtnLdpX2y3ugWgXgg6AOBnxSeqz10Ey7y1fLfGpm6SO9BvdIR6IegAgB/Y7YLWuu6NCcLvrp5dvFWfZOXr6+2HrG4FfkDQAQA/qKh2n7soyBlj9Nl3+dp5sNTqVhpE2Y9OwIZ9cHk5APjB7JV7rG6hQR0uray1LHV9riYt3CyJ+9EgcDGjAwB+cOgMwcBuToUcu7Db1404iaADAGgwZAUEGoIOAPgBB/zgMPxvK6xuAX5G0AEAv7B30nH9xKXYwbbXm3mQqe0RdADAD+w+o/PZd/lWtwDUCUEHAFAnPz5Zd92eYxZ2AtQdQQcA/MCOEzoLNx7w/O/icvvd7fmz7wqsbgF+QNABANTJks0ng0D2AacW2zAUZOQctLoF+AFBBwD8wM73ZHln1V6rW/Cb4hNVVreABkbQAQA/sOvzIZ/+dIs+zMz7yfXBHvD+/MkWq1tAA+MREADgB8F9uD+znIIS/Xvb2R98Gez7vYXLzW2HGR0AQJ0UOCusbgHwGUEHAPxg/9Eyq1sAIIIOAPjF3qMnrG4BgAg6ANDgUtftt7oFAP9B0AGABuRyG01cuNnqNixzotJldQuAF4IOADSg215baXULlpqesd3qFgAvBB0AaEDf5jXty5M/yeJhnwgsBB0AAGBbBB0AAGBbBB0AAGBbBB0AAGBbBB0AQIOoqnFb3QJQC0EHABqIy66PLK+jtC2FVrcA1MLTywGggfz+w2+tbsEy763eqz9/ssXqNoBamNEBgHOodrmVum6/9p/l+VVFZVVauOlAI3YVWOwScnYfKdMnWU33/0c7IugAwDm8vWKPJi7crF9P/fona678v4xG7Aj+NDY1y+oW0IAIOgBwDst2HLa6BQD1RNABgHNYtevoWddztREQuDgZGQDOInNf0VnXv/LvnZqRsaORugHgK2Z0AOAsbn991VnXE3KAwEbQAQDgNBXVLqtbQAMh6AAAcJqvth2yugU0EM7RAQAfHT1eqbvfWqueHRxWtwLgHJjRAdAoPt9coP/vH+t05Hil1a2ctz99nK1thaX654Y8q1uBnzz7WY7yin76BpEIHgQdAI3it3M3atmOw5q8ZKvVrZy3c12JheCX76zQfbPXW90GGgBBB0CjWrgxuG+v/5d/5ehQafDPSuHcdh46bnULaAAEHQCNbkFmXlA86bu8qvaVN/9YuceCTgDUF0EHQKMb/+G3+igz1+o2zunz7AKrW4DFjAn8QI6zI+gAsMSGvYF9novLbfT3ZbutbgMWK3BWWN0CzhNBBwDOYMHGPG0rLLW6DVjMzYxO0CPoAMAZ7DxIyIFEzgl+3DAQgN88+Um2Nu0v1n0DLrG6FQBNFEEHgF+43Ebvrt4nSXrsn99a3I1v/vVtvt5cztVVgB3w1RUAnGb0/E1Wt4AAwVdXwY+gA8Av9h87++3zQ0IaqREfVdW4rW4BAcSIpBPsCDoA/OLpT7dY3UK9DJr6tdUtIIAE+4xORbVLkxZu1tdN+GnsBB0ADa7QWaHtQXhptsttlM99U2Ajby3frfnr9uu+OU33uV2cjAygwfWd8m+rW6iXx/6ZZXULCDBBPqGjPUd4AjtBB0CTV17l0ktf7tAnWflWt4IAE8yPgHC7jRZszPP8fLyyRpHNQ9UsRAoJ1JPk/ICgA6BBnaiqsbqFOnG7jZo1O/mPfbcn0yzuBoEq2GLO1oIStYkKV2x0Cz0yb6PXuvV7jum+OevV99I2Sn2wX50+7+vthxQR1kz9L2vnj3YbBUEHQIN69evv61QXIuv+onx/zT796eNsxUW3UGEJ5+Tgp/3UhM63ucVq1SJMl154QeM29BNW7zqqZxfnaEt+iSRp4aj++jy70Kvm1Hk6a3Yfq9NnFp+o0n2zT77n++duVnm1S1sLStW7U2vPHwnBgJORATSovUcD/5yAP32cLUmEHJzTHxdtrrWs0Fmh37y6UtdPX6pq1w+3IzhUWqF1e+oWIurK7Tb6KDNPuw4fP2vd/7y5xhNyJOm211ad97ZLK36Yna12Gd3x+mrdOWu1PtiQ+5PvydxXpPH//FaHSyvPe/sNhaAThIwxOsQ/0AhQbnewTfYDP+1MwWX3kR9CR69n0lV8okqSdPVz/9ads1Zr1fdHdKC4XNe+8JXeWr671vvLq1xav/eYXOf4b8V5olrvrdmnCR9+qxumLz3PPfH2x0Wbdaj0h+NI7rETem/1XlVUuzzLwkJ/mLWpdru1/T/Pf5u08GT4O3q80qtekm5/fZUWbMzTn//zx0Qg4Ksrixhj9Mi8jYqNbqGnhl/h03vH//NbLdx0QK/dfZVu6dHeTx2iqSg+UaXXv9mlKpdbI3rF68qLW0s6GViW7jishIscurBVRJ0/r6aOQceqp0IH88mlCBA/+hU6UeXS59mF+p+rL/YsW/H9Ec1dt1+5x8r17OKt+n8DOquyxq3I8FBJ0h1vrNKW/BJd3Kal0sZdq5bh3ofiorIqPfDuBmXuK/JafqKqRi9m7FCvjj/T7sNl6twuSqPnb6rXs+Tmrd2veWv3S5I+G32N7py1WieqXPrzJyfvfzXnvl8pPPSHuZCeT6d7vT+/uFz9n//K83OPixz61+hrPD+nbSmUMSYgTnoOMU34v/qSkhI5HA45nU5FR0c33nYrqnWgqFw3v7xckrTs99fp8PEKJVzkUERY6Dnff8nExZKk7u2j9erdVykqIlQxrVr4tWfY16nfp1Nuv6qDvj9Uqv+5+mJN/M9fbiEh0hfjfq04RwtFt2juqS2vcimv6ITCQpvpnVV79fhNXfXovE36qo43J9v7/NCG25E6+igzTxM+DK5nb8Fas1ISJZ38N7dF81D96rkvvdbHtIrQHYkd9No3uyRJyX0u1pLNBSo+US1J+mXHnykrt1iDul6oCy+I0IeZeV7vn5l8pa6Id+iCiDBd2CpCkxZ+p/nrfvrroUDVNipcR8uqPD9HhYdq0SMDdHlsqwbfli/Hb4JOIwWdo8cr1bpluCYu/E7/3JCn3p1aa8NpaV2Spv13L92R2MFrWUW1S5XVbjlanjzAnDow/fhEytMPGJU1rjqFJtifMUZuI4X+xMmDpwedc9n7/FDlFZ3Q55sL9dySrbXW97jIoc0HnHX6rI1/Hqw2UeHnrDtYUqH0nIO67cqL1KJ56E/uy9k8tzhHizYdUFREmPYFwXlEaJru7N1B/9yQd+7CIOKPP2gIOnXkr6Djdhs9868tuizmAi3+rkBrz+PktDZR4Tr2n4S84Lf99eQn2V4nnJ2ye/It2n/shAZN+8ZreXKfi9W5bZQeuLZznaYQA2WqEb6rrHFpxc4j6ntpW0VFnJwKd7mNkt9cowJnhf49fqCah9Y+Lc/XoHN15zYNdsLl7268XI9cd5nCQpup2uVWWLMQz+/f3iNlem7JVg25Iq7WDMwnjwxQnKOFYlpFeOqrXW59l1esnh1+Vms/l+44rHv+sa5Begbgm/kj+6rfZW0b9DMJOnXkr6Bz66srlZVb3GCf15C+fOzX+nnMmacRP8k6oP/7LEezUhKV2KlNI3fmP9Uut/KLy9WpbVStdSUV1co+4FTfzm0D8nLJapdb3+aePHhXudy6IOKH7/KNMfowM0+Pf/SdRl7bWW8u33POzxvRK17ZB5wa3D1WWbnF2lpQopKKwLrvza7Jt+i5xVv1j5Xn3p9TEi6KVvaBH/4AaBURpotaR+rFu36pT7Ly9cbSXf5oFUAdNfSsTpMKOq+99pqmTp2qgoICXXHFFXrppZd07bXX1um9/go6vv6F3Nj+9eg16ta+lWrcRi2ahyqv6IRKymt0yyvLPTXRLcL0j3t/pQ/W56q9o4VGXfdzHSqpVMc2kT7N+FRUu9QsJER/+2qn8osrNKxne7X/WQuVVdaovSNS8T+L1Kff5mvM/E2SpL/9z5VqExWu7/Kcqna51Ty0mf6atk3/2/di/e7Gy7Xz0HG1iQpXuwsi9NW2kzeympa+Xbdf1UFdYi5QtdvoyU+yPd+N/35IV039YrskafJ/9dBvfhmvluE/fKU3aNo3nq8x3rv/al3b5cIz7seJqho9u3irbk6IU7f20Xryk2xd1zVGw3rGKyKsmWrcRuFh3rMIq3cd1axlu1R0olpPDuuuxE6tVVJRrWU7Diu/uFxHjldp7Z5j+vY/oXjBb/srIqyZ3ly+Wz+LbK53Vu+r1cfIaztrW2Gp2kSFcxdfAEGDoFNPH3zwgVJSUvTaa69pwIABmjVrlt566y3l5OTo4osvPuf7m2rQOR+RzUOV8div1bpluA4Ul+uFtG3akl+i//tNgn7/0bcq+k/AAADgFIJOPfXp00dXXXWVXn/9dc+ybt266dZbb9WUKVPO+X6CDgAA/mdl0AnaGwZWVVUpMzNTSUlJXsuTkpK0atWZ7whZWVmpkpISrxcAALCvoA06R44ckcvlUmxsrNfy2NhYFRYWnvE9U6ZMkcPh8Lw6duzol95uu+oiv3wuAADwTdAGnVNOPzH2bJdHT5o0SU6n0/PKzfXPDZmm/3cvv3wuAADBJv13v7Z0+0H7CIh27dopNDS01uzNoUOHas3ynBIREaGIiLrfyr6+QkJCLLnjKwAA8Ba0Mzrh4eFKTExURkaG1/KMjAz179/foq4AAEAgCdoZHUl67LHHlJKSot69e6tfv376+9//rv379+vhhx+2ujUAABAAgjro3HXXXTp69Kj+8pe/qKCgQAkJCVqyZIk6depkdWsAACAABPV9dM6XVU8vBwAA9dck7qMDAABwLgQdAABgWwQdAABgWwQdAABgWwQdAABgWwQdAABgWwQdAABgWwQdAABgWwQdAABgW0H9CIjzdeqm0CUlJRZ3AgAA6urUcbsuD3do0kGntLRUktSxY0eLOwEAAL4qLS2Vw+E4a02TftaV2+1Wfn6+WrVqpZCQkAb97JKSEnXs2FG5ubk8R8tHjF39MXbnh/GrP8au/hg73xljVFpaqvj4eDVrdvazcJr0jE6zZs3UoUMHv24jOjqaX9x6Yuzqj7E7P4xf/TF29cfY+eZcMzmncDIyAACwLYIOAACwLYKOn0REROipp55SRESE1a0EHcau/hi788P41R9jV3+MnX816ZORAQCAvTGjAwAAbIugAwAAbIugAwAAbIugAwAAbIugU0+vvfaaOnfurBYtWigxMVHLly8/a/3SpUuVmJioFi1a6NJLL9Ubb7zRSJ0GJl/Gb+HChRo8eLAuvPBCRUdHq1+/fvriiy8asdvA4uvv3ikrV65UWFiYfvnLX/q3wQDm69hVVlbqiSeeUKdOnRQREaHLLrtM//jHPxqp28Dj6/jNnTtXvXr1UsuWLdW+fXvdd999Onr0aCN1GziWLVum4cOHKz4+XiEhIfr444/P+R6OGQ3IwGepqammefPm5s033zQ5OTlm7NixJioqyuzbt++M9bt37zYtW7Y0Y8eONTk5OebNN980zZs3Nx999FEjdx4YfB2/sWPHmr/+9a9m3bp1ZseOHWbSpEmmefPmZuPGjY3cufV8HbtTiouLzaWXXmqSkpJMr169GqfZAFOfsRsxYoTp06ePycjIMHv27DFr1641K1eubMSuA4ev47d8+XLTrFkz8/LLL5vdu3eb5cuXmyuuuMLceuutjdy59ZYsWWKeeOIJs2DBAiPJLFq06Kz1HDMaFkGnHq6++mrz8MMPey37xS9+YSZOnHjG+scff9z84he/8Fr20EMPmb59+/qtx0Dm6/idSffu3c0zzzzT0K0FvPqO3V133WX+9Kc/maeeeqrJBh1fx+7zzz83DofDHD16tDHaC3i+jt/UqVPNpZde6rXslVdeMR06dPBbj8GgLkGHY0bD4qsrH1VVVSkzM1NJSUley5OSkrRq1aozvmf16tW16ocMGaINGzaourrab70GovqM3+ncbrdKS0vVpk0bf7QYsOo7drNnz9auXbv01FNP+bvFgFWfsfv000/Vu3dvvfDCC7rooot0+eWXa8KECSovL2+MlgNKfcavf//+ysvL05IlS2SM0cGDB/XRRx9p6NChjdFyUOOY0bCa9EM96+PIkSNyuVyKjY31Wh4bG6vCwsIzvqewsPCM9TU1NTpy5Ijat2/vt34DTX3G73TTp09XWVmZ7rzzTn+0GLDqM3Y7d+7UxIkTtXz5coWFNd3/3Oszdrt379aKFSvUokULLVq0SEeOHNGoUaN07NixJneeTn3Gr3///po7d67uuusuVVRUqKamRiNGjNDf/va3xmg5qHHMaFjM6NRTSEiI18/GmFrLzlV/puVNha/jd8r8+fP19NNP64MPPlBMTIy/2gtodR07l8ul5ORkPfPMM7r88ssbq72A5svvndvtVkhIiObOnaurr75at9xyi2bMmKE5c+Y0yVkdybfxy8nJ0ZgxY/Tkk08qMzNTaWlp2rNnjx5++OHGaDXoccxoOE33T7x6ateunUJDQ2v9FXPo0KFaCfyUuLi4M9aHhYWpbdu2fus1ENVn/E754IMPdP/99+vDDz/UjTfe6M82A5KvY1daWqoNGzZo06ZNevTRRyWdPHgbYxQWFqb09HRdf/31jdK71erze9e+fXtddNFFcjgcnmXdunWTMUZ5eXnq0qWLX3sOJPUZvylTpmjAgAH6/e9/L0nq2bOnoqKidO211+rZZ59lVuIsOGY0LGZ0fBQeHq7ExERlZGR4Lc/IyFD//v3P+J5+/frVqk9PT1fv3r3VvHlzv/UaiOozftLJmZx7771X8+bNa7Lf8fs6dtHR0dq8ebOysrI8r4cfflhdu3ZVVlaW+vTp01itW64+v3cDBgxQfn6+jh8/7lm2Y8cONWvWTB06dPBrv4GmPuN34sQJNWvmfYgJDQ2V9MPsBM6MY0YDs+gk6KB26jLLt99+2+Tk5Jhx48aZqKgos3fvXmOMMRMnTjQpKSme+lOXCv7ud78zOTk55u23327Slwr6On7z5s0zYWFh5tVXXzUFBQWeV3FxsVW7YBlfx+50TfmqK1/HrrS01HTo0MHccccdZsuWLWbp0qWmS5cu5oEHHrBqFyzl6/jNnj3bhIWFmddee83s2rXLrFixwvTu3dtcffXVVu2CZUpLS82mTZvMpk2bjCQzY8YMs2nTJs+l+Rwz/IugU0+vvvqq6dSpkwkPDzdXXXWVWbp0qWfdPffcYwYOHOhV/80335grr7zShIeHm0suucS8/vrrjdxxYPFl/AYOHGgk1Xrdc889jd94APD1d+/HmnLQMcb3sdu6dau58cYbTWRkpOnQoYN57LHHzIkTJxq568Dh6/i98sorpnv37iYyMtK0b9/e3H333SYvL6+Ru7be119/fdZ/wzhm+FeIMcwhAgAAe+IcHQAAYFsEHQAAYFsEHQAAYFsEHQAAYFsEHQAAYFsEHQAAYFsEHQAAYFsEHQAA0KCWLVum4cOHKz4+XiEhIfr44499/gxjjKZNm6bLL79cERER6tixoyZPnuzz5/BQTwAA0KDKysrUq1cv3Xfffbr99tvr9Rljx45Venq6pk2bph49esjpdOrIkSM+fw53RgYAAH4TEhKiRYsW6dZbb/Usq6qq0p/+9CfNnTtXxcXFSkhI0F//+lcNGjRIkrR161b17NlT2dnZ6tq163ltn6+uAABAo7rvvvu0cuVKpaam6rvvvtN///d/66abbtLOnTslSf/617906aWX6rPPPlPnzp11ySWX6IEHHtCxY8d83hZBBwAANJpdu3Zp/vz5+vDDD3Xttdfqsssu04QJE3TNNddo9uzZkqTdu3dr3759+vDDD/Xuu+9qzpw5yszM1B133OHz9jhHBwAANJqNGzfKGKPLL7/ca3llZaXatm0rSXK73aqsrNS7777rqXv77beVmJio7du3+/R1FkEHAAA0GrfbrdDQUGVmZio0NNRr3QUXXCBJat++vcLCwrzCULdu3SRJ+/fvJ+gAAIDAdOWVV8rlcunQoUO69tprz1gzYMAA1dTUaNeuXbrsssskSTt27JAkderUyaftcdUVAABoUMePH9f3338v6WSwmTFjhq677jq1adNGF198sf73f/9XK1eu1PTp03XllVfqyJEj+uqrr9SjRw/dcsstcrvd+tWvfqULLrhAL730ktxutx555BFFR0crPT3dp14IOgAAoEF98803uu6662otv+eeezRnzhxVV1fr2Wef1bvvvqsDBw6obdu26tevn5555hn16NFDkpSfn6/Ro0crPT1dUVFRuvnmmzV9+nS1adPGp14IOgAAwLa4vBwAANgWQQcAANgWQQcAANgWQQcAANgWQQcAANgWQQcAANgWQQcAANgWQQcAANgWQQcAANgWQQcAANgWQQcAANgWQQcAANjW/w91wqk8nji45QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "plt.plot(range(len(losses)), [l.detach().numpy() for l in losses])\n",
    "\n",
    "#print(np.min(losses), axis = 1)\n",
    "\n",
    "print(losses[len(losses)-1])\n",
    "\n",
    "#torch.save(policy_network, 'stick_game.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283.92980706099115\n",
      "290.73968600879505\n",
      "261.0219016345902\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m state, reward, ended, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample())\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ended :\n\u001b[0;32m---> 13\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#state, reward, ended, info = env.step(env.action_space.sample())\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([state], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gym/core.py:343\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Renders the environment with kwargs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gym/wrappers/env_checker.py:57\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m passive_env_render_check(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gym/envs/box2d/lunar_lander.py:672\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    670\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 672\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m    676\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    677\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    episodes = 2000\n",
    "    rewards = []\n",
    "\n",
    "\n",
    "    for episode in range(episodes) :\n",
    "        state = env.reset()\n",
    "        ended = False\n",
    "        rewardsum = 0\n",
    "        #state, reward, ended, info = env.step(env.action_space.sample())\n",
    "        state, reward, ended, info = env.step(env.action_space.sample())\n",
    "        while not ended :\n",
    "            env.render()\n",
    "            #state, reward, ended, info = env.step(env.action_space.sample())\n",
    "            state_tensor = torch.tensor([state], dtype=torch.float32)\n",
    "            q_values = policy_network(state_tensor)\n",
    "            action = torch.argmax(q_values).item()\n",
    "\n",
    "            state, reward, ended, info = env.step(action)\n",
    "            rewardsum += reward\n",
    "        print(rewardsum)\n",
    "        t.sleep(1)\n",
    "        rewards.append(rewardsum)\n",
    "\n",
    "print(np.max(rewards))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
